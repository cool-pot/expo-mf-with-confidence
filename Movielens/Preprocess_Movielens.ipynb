{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python2\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Thu May  4 14:43:35 2017\n",
    "\n",
    "@author: zhouyi\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "import json\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Change this to wherever you keep the data\n",
    "DATA_DIR = '/Users/zhouyi/Documents/GraduatePJ/ml-1m'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/zhouyi/anaconda/lib/python2.7/site-packages/ipykernel/__main__.py:1: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  if __name__ == '__main__':\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After filtering, there are 883466 triplets from 4200 users and 2019 movies (sparsity level 10.418%)\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_table(os.path.join(DATA_DIR, 'ratings.dat'), header=None, sep='::', names=['uid', 'sid', 'rating','timestamp'])\n",
    "df = df[['uid','sid','rating']]\n",
    "def get_count(df, id):\n",
    "    playcount_groupbyid = df[[id, 'rating']].groupby(id, as_index=False)\n",
    "    count = playcount_groupbyid.size()\n",
    "    return count\n",
    "\n",
    "def filter_triplets(df, min_sc=100,min_uc=50):\n",
    "    # Only keep the triplets for songs which were listened to by at least min_sc users. \n",
    "    songcount = get_count(df, 'sid')\n",
    "    df = df[df['sid'].isin(songcount.index[songcount >= min_sc])]\n",
    "    \n",
    "    \n",
    "    # Only keep the triplets for users which which at least listened min_uc songs \n",
    "    songcount = get_count(df, 'uid')\n",
    "    df = df[df['uid'].isin(songcount.index[songcount >= min_uc])]\n",
    "    \n",
    "    # Update both usercount and songcount after filtering\n",
    "    usercount, songcount = get_count(df, 'uid'), get_count(df, 'sid') \n",
    "    return df, usercount, songcount\n",
    "\n",
    "\n",
    "df, usercount, songcount = filter_triplets(df)\n",
    "\n",
    "sparsity_level = float(df.shape[0]) / (usercount.shape[0] * songcount.shape[0])\n",
    "print \"After filtering, there are %d triplets from %d users and %d movies (sparsity level %.3f%%)\" % (df.shape[0], \n",
    "                                                                                                      usercount.shape[0], \n",
    "                                                                                                      songcount.shape[0], \n",
    "                                                                                                      sparsity_level * 100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are total of 4200 unique users in the training set and 4200 unique users in the entire dataset\n",
      "There are total of 2019 unique items in the training set and 2019 unique items in the entire dataset\n",
      "There are total of 4200 unique users in the training set and 4200 unique users in the entire dataset\n",
      "There are total of 2019 unique items in the training set and 2019 unique items in the entire dataset\n"
     ]
    }
   ],
   "source": [
    "#Generate train/test/vad sets\n",
    "\n",
    "np.random.seed(12345)\n",
    "n_ratings = df.shape[0]\n",
    "test = np.random.choice(n_ratings, size=int(0.20 * n_ratings), replace=False)\n",
    "test_idx = np.zeros(n_ratings, dtype=bool)\n",
    "test_idx[test] = True\n",
    "\n",
    "test_df = df[test_idx]\n",
    "train_df = df[~test_idx]\n",
    "\n",
    "##Make sure there is no empty row/column in the training data\n",
    "print \"There are total of %d unique users in the training set and %d unique users in the entire dataset\" % \\\n",
    "(len(pd.unique(train_df['uid'])), len(pd.unique(df['uid'])))\n",
    "print \"There are total of %d unique items in the training set and %d unique items in the entire dataset\" % \\\n",
    "(len(pd.unique(train_df['sid'])), len(pd.unique(df['sid'])))\n",
    "\n",
    "np.random.seed(13579)\n",
    "n_ratings = train_df.shape[0]\n",
    "vad = np.random.choice(n_ratings, size=int(0.10 * n_ratings), replace=False)\n",
    "vad_idx = np.zeros(n_ratings, dtype=bool)\n",
    "vad_idx[vad] = True\n",
    "\n",
    "vad_df = train_df[vad_idx]\n",
    "train_df = train_df[~vad_idx]\n",
    "\n",
    "##Make sure there is no empty row/column in the training data\n",
    "print \"There are total of %d unique users in the training set and %d unique users in the entire dataset\" % \\\n",
    "(len(pd.unique(train_df['uid'])), len(pd.unique(df['uid'])))\n",
    "print \"There are total of %d unique items in the training set and %d unique items in the entire dataset\" % \\\n",
    "(len(pd.unique(train_df['sid'])), len(pd.unique(df['sid'])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#unique uid,sid\n",
    "unique_uid = sorted(pd.unique(df['uid']))\n",
    "unique_sid = sorted(pd.unique(df['sid']))\n",
    "\n",
    "with open(os.path.join(DATA_DIR, 'unique_uid.txt'), 'w') as f:\n",
    "    for uid in unique_uid:\n",
    "        f.write('%s\\n' % uid)\n",
    "\n",
    "\n",
    "with open(os.path.join(DATA_DIR, 'unique_sid.txt'), 'w') as f:\n",
    "    for sid in unique_sid:\n",
    "        f.write('%s\\n' % sid)\n",
    "        \n",
    "\n",
    "uid2idx = dict((uid, idx) for (idx, uid) in enumerate(unique_uid))\n",
    "sid2idx = dict((sid, idx) for (idx, sid) in enumerate(unique_sid))\n",
    "\n",
    "with open(os.path.join(DATA_DIR, 'sid2idx.json'), 'w') as f:\n",
    "    json.dump(sid2idx, f)\n",
    "    \n",
    "with open(os.path.join(DATA_DIR, 'uid2idx.json'), 'w') as f:\n",
    "    json.dump(uid2idx, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/zhouyi/anaconda/lib/python2.7/site-packages/ipykernel/__main__.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/Users/zhouyi/anaconda/lib/python2.7/site-packages/ipykernel/__main__.py:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "#Numerize the data into (user_index, item_index, count) format\n",
    "uid = map(lambda x: uid2idx[x], train_df['uid'])\n",
    "sid = map(lambda x: sid2idx[x], train_df['sid'])\n",
    "train_df['uid'] = uid\n",
    "train_df['sid'] = sid\n",
    "train_df.to_csv(os.path.join(DATA_DIR, 'train.num.csv'), index=False)\n",
    "\n",
    "\n",
    "uid = map(lambda x: uid2idx[x], test_df['uid'])\n",
    "sid = map(lambda x: sid2idx[x], test_df['sid'])\n",
    "test_df['uid'] = uid\n",
    "test_df['sid'] = sid\n",
    "test_df.to_csv(os.path.join(DATA_DIR, 'test.num.csv'), index=False)\n",
    "\n",
    "uid = map(lambda x: uid2idx[x], vad_df['uid'])\n",
    "sid = map(lambda x: sid2idx[x], vad_df['sid'])\n",
    "vad_df['uid'] = uid\n",
    "vad_df['sid'] = sid\n",
    "vad_df.to_csv(os.path.join(DATA_DIR, 'vad.num.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
